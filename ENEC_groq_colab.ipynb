{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmerajesh4990/AIpracticeandtraining/blob/main/ENEC_groq_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56k-BRqXWFOo",
        "outputId": "8c72fffe-872f-4607-9cba-d5741fe950a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.37.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymGssEbJi8NG",
        "outputId": "8df1be95-af3c-45aa-f055-8ddf80036d89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnYZupvAV9U9",
        "outputId": "77e46c1a-fa29-4c2e-a4a0-89a5164e0138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM can refer to several things, but I'll cover the most common meaning related to technology.\n",
            "\n",
            "**LLM:** Large Language Model\n",
            "\n",
            "LLMs are a type of artificial intelligence (AI) designed to process natural language data. They're trained on vast amounts of text data, enabling them to understand, generate, and respond to human language in a way that simulates human-like conversation.\n",
            "\n",
            "Large language models are based on neural networks, a type of machine learning algorithm. They use complex algorithms to analyze text patterns, relationships, and contextual clues to produce human-like output. LLMs can perform a range of tasks, including:\n",
            "\n",
            "1. **Content generation:** Writing articles, stories, or even entire books.\n",
            "2. **Language translation:** Translating text from one language to another.\n",
            "3. **Text summarization:** Summarizing long pieces of text into concise, meaningful summaries.\n",
            "4. **Question answering:** Answering questions based on a given text or knowledge base.\n",
            "5. **Chatbots and conversational AI:** Engaging in conversations, responding to queries, and providing customer support.\n",
            "\n",
            "Some popular examples of LLMs include:\n",
            "\n",
            "1. **Chatbots:** Many online chatbots, like customer support chatbots or virtual assistants like Siri, Alexa, or Google Assistant, use LLMs to understand and respond to user queries.\n",
            "2. **Language translation tools:** Google Translate, Microsoft Translator, and other translation platforms rely on LLMs to translate text between languages.\n",
            "3. **Writing assistants:** LLMs are also used in writing tools like Grammarly, Hemingway Editor, and other proofreading software that helps writers improve their writing style and clarity.\n",
            "\n",
            "While LLMs have made tremendous progress in recent years, they still have limitations, such as:\n",
            "\n",
            "1. **Contextual understanding:** While LLMs can generate human-like responses, they may not always understand the context or nuances of a conversation.\n",
            "2. **Bias and accuracy:** LLMs can perpetuate biases present in the training data, leading to inaccurate or misleading responses.\n",
            "3. **Scalability:** Training and deploying LLMs can be computationally expensive and require significant resources.\n",
            "\n",
            "However, the progress made in LLM research has been remarkable, and continued improvements in the field are expected to lead to even more sophisticated AI models capable of solving complex language-related tasks.\n",
            "\n",
            "Would you like me to expand on any of these points or address a specific aspect of LLM technology?\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from google.colab import userdata  # Secure storage for API keys in Colab\n",
        "\n",
        "\n",
        "\n",
        "# Get the Groq API key securely (use userdata in Colab)\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")  # Store using userdata.set(\"GROQ_API_KEY\", \"your_api_key\")\n",
        "\n",
        "# Define the URL for the Groq API endpoint\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Set the headers for the API request\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {groq_api_key}\"\n",
        "}\n",
        "\n",
        "# Define the body for the API request\n",
        "body = {\n",
        "    \"model\": \"llama-3.1-8b-instant\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is llm?\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Send a POST request to the Groq API\n",
        "response = requests.post(url, headers=headers, json=body)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    print(response.json()['choices'][0]['message']['content'])\n",
        "else:\n",
        "    print(\"Error:\", response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.json())"
      ],
      "metadata": {
        "id": "KEkeyrGUha-6",
        "outputId": "c8ef8ba9-12e7-4c70-9ef1-3be31aa900bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-d81a7acf-39c8-4b01-9e2b-fe5f27caa942', 'object': 'chat.completion', 'created': 1765446174, 'model': 'llama-3.1-8b-instant', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"LLM can refer to several things, but I'll cover the most common meaning related to technology.\\n\\n**LLM:** Large Language Model\\n\\nLLMs are a type of artificial intelligence (AI) designed to process natural language data. They're trained on vast amounts of text data, enabling them to understand, generate, and respond to human language in a way that simulates human-like conversation.\\n\\nLarge language models are based on neural networks, a type of machine learning algorithm. They use complex algorithms to analyze text patterns, relationships, and contextual clues to produce human-like output. LLMs can perform a range of tasks, including:\\n\\n1. **Content generation:** Writing articles, stories, or even entire books.\\n2. **Language translation:** Translating text from one language to another.\\n3. **Text summarization:** Summarizing long pieces of text into concise, meaningful summaries.\\n4. **Question answering:** Answering questions based on a given text or knowledge base.\\n5. **Chatbots and conversational AI:** Engaging in conversations, responding to queries, and providing customer support.\\n\\nSome popular examples of LLMs include:\\n\\n1. **Chatbots:** Many online chatbots, like customer support chatbots or virtual assistants like Siri, Alexa, or Google Assistant, use LLMs to understand and respond to user queries.\\n2. **Language translation tools:** Google Translate, Microsoft Translator, and other translation platforms rely on LLMs to translate text between languages.\\n3. **Writing assistants:** LLMs are also used in writing tools like Grammarly, Hemingway Editor, and other proofreading software that helps writers improve their writing style and clarity.\\n\\nWhile LLMs have made tremendous progress in recent years, they still have limitations, such as:\\n\\n1. **Contextual understanding:** While LLMs can generate human-like responses, they may not always understand the context or nuances of a conversation.\\n2. **Bias and accuracy:** LLMs can perpetuate biases present in the training data, leading to inaccurate or misleading responses.\\n3. **Scalability:** Training and deploying LLMs can be computationally expensive and require significant resources.\\n\\nHowever, the progress made in LLM research has been remarkable, and continued improvements in the field are expected to lead to even more sophisticated AI models capable of solving complex language-related tasks.\\n\\nWould you like me to expand on any of these points or address a specific aspect of LLM technology?\"}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'queue_time': 0.205924018, 'prompt_tokens': 40, 'prompt_time': 0.003237817, 'completion_tokens': 495, 'completion_time': 0.926961436, 'total_tokens': 535, 'total_time': 0.930199253}, 'usage_breakdown': None, 'system_fingerprint': 'fp_4387d3edbb', 'x_groq': {'id': 'req_01kc6cngz7esqrxm3f7kg9ghvj', 'seed': 1786066596}, 'service_tier': 'on_demand'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCnSTr5bmCVQ",
        "outputId": "f299a550-051b-4d0a-ce0d-4b98c70ba59a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import gradio as gr\n",
        "\n",
        "from google.colab import userdata  # Secure storage for API keys in Colab\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Define the URL for the Groq API endpoint\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Set the headers for the API request\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {groq_api_key}\"\n",
        "}\n",
        "\n",
        "# Function to interact with Groq API\n",
        "def chat_with_groq(user_input):\n",
        "    body = {\n",
        "        \"model\": \"openai/gpt-oss-120b\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=body)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.json()}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=chat_with_groq,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything...\"),\n",
        "    outputs=gr.Textbox(lines=5),\n",
        "    title=\"DDS Chat with Groq AI (GPT OSS 120B)\",\n",
        "    description=\"Type your question below and get a response powered by Groq's Llama 3.1-8B model.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio app\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "D627mfvelNaO",
        "outputId": "b65e636e-00d5-4042-d5bb-5cad5cabf0a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d34a4f801b3777424d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d34a4f801b3777424d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for hugging face  u need to set env seperately and save a app.py file\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import gradio as gr\n",
        "\n",
        "# Retrieve the API key from the environment variable\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "# Define the API endpoint and headers\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "headers = {\"Authorization\": f\"Bearer {groq_api_key}\"}\n",
        "\n",
        "# Function to interact with Groq API\n",
        "def chat_with_groq(user_input):\n",
        "    body = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=body)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.json()}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=chat_with_groq,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything...\"),\n",
        "    outputs=gr.Textbox(lines=5),\n",
        "    title=\"Chat with Groq AI (Llama 3.1-8B)\",\n",
        "    description=\"Type your question below and get a response powered by Groq's Llama 3.1-8B model.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()"
      ],
      "metadata": {
        "id": "M0o0K7pXnZKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "670a87ef-cf79-49a7-a274-21afc17a9e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dbed026f96bf47a235.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dbed026f96bf47a235.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}