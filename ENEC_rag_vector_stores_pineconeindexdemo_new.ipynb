{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmerajesh4990/AIpracticeandtraining/blob/main/ENEC_rag_vector_stores_pineconeindexdemo_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
      "metadata": {
        "id": "307804a3-c02b-4a57-ac0d-172c30ddc851"
      },
      "source": [
        "# Pinecone Vector Store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36be66bf",
      "metadata": {
        "id": "36be66bf"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddff1e4",
      "metadata": {
        "id": "9ddff1e4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install llama-index llama-index-vector-stores-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3KbgDaxfBa9"
      },
      "id": "h3KbgDaxfBa9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48af8e1",
      "metadata": {
        "id": "d48af8e1"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
      "metadata": {
        "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396"
      },
      "source": [
        "#### Creating a Pinecone Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a",
      "metadata": {
        "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the OpenAI API key from Google Colab secrets\n",
        "openai.api_key = userdata.get('openai')\n",
        "\n",
        "if openai.api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "3mf6ed5pBFwj"
      },
      "id": "3mf6ed5pBFwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad14111-0bbb-4c62-906d-6d6253e0cdee",
      "metadata": {
        "id": "4ad14111-0bbb-4c62-906d-6d6253e0cdee"
      },
      "outputs": [],
      "source": [
        "#PUT Tyour api key\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "if api_key:\n",
        "    os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key"
      ],
      "metadata": {
        "id": "ftxZ6VHsVpsX"
      },
      "id": "ftxZ6VHsVpsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c90087-bdd9-4ca4-b06b-2af883559f88",
      "metadata": {
        "id": "c2c90087-bdd9-4ca4-b06b-2af883559f88",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "index_name = \"quickstart\"\n",
        "\n",
        "# Delete index if exists\n",
        "if index_name in [idx[\"name\"] for idx in pc.list_indexes()]:\n",
        "    pc.delete_index(index_name)\n",
        "\n",
        "# dimensions are for text-embedding-ada-002\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=1536,\n",
        "    metric=\"euclidean\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667f3cb3-ce18-48d5-b9aa-bfc1a1f0f0f6",
      "metadata": {
        "id": "667f3cb3-ce18-48d5-b9aa-bfc1a1f0f0f6"
      },
      "outputs": [],
      "source": [
        "pinecone_index = pc.Index(\"quickstart\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
      "metadata": {
        "id": "8ee4473a-094f-4d0a-a825-e1213db07240"
      },
      "source": [
        "#### Load documents, build the PineconeVectorStore and VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2bcc07",
      "metadata": {
        "id": "0a2bcc07"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d782f76",
      "metadata": {
        "id": "7d782f76"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
      "metadata": {
        "id": "68cbd239-880e-41a3-98d8-dbb3fab55431"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "Jv678Qg8x1HG",
        "collapsed": true
      },
      "id": "Jv678Qg8x1HG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1558b3",
      "metadata": {
        "id": "ba1558b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f07c3fd"
      },
      "source": [
        "The previous error occurred because the variable `pinecone_index` was used before it was defined. To fix this, I have combined the code that defines `pinecone_index` and the code that uses it into a single cell."
      ],
      "id": "5f07c3fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dd682a6"
      },
      "source": [
        "# Initialize Pinecone index\n",
        "pinecone_index = pc.Index(\"quickstart\")\n",
        "\n",
        "# initialize without metadata filter\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")"
      ],
      "id": "4dd682a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "04304299-fc3e-40a0-8600-f50c3292767e",
      "metadata": {
        "id": "04304299-fc3e-40a0-8600-f50c3292767e"
      },
      "source": [
        "#### Query Index\n",
        "\n",
        "May take a minute or so for the index to be ready!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35369eda",
      "metadata": {
        "id": "35369eda"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "start_time = time.time()\n",
        "response = query_engine.query(\"what is this document about\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "\n",
        "# End timer and print duration\n",
        "end_time = time.time()\n",
        "print(f\"\\nExecution Time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
      "metadata": {
        "id": "bedbb693-725f-478f-be26-fa7180ea38b2"
      },
      "outputs": [],
      "source": [
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "EgDEgB0_cOYo",
        "collapsed": true
      },
      "id": "EgDEgB0_cOYo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Imports ---\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "\n",
        "# New import for OpenAI LLM\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# --- Initialize Pinecone ---\n",
        "\n",
        "index_name = \"quickstart\"\n",
        "dimension = 1536\n",
        "\n",
        "# Delete index if exists (optional: mirrors original behavior)\n",
        "if index_name in [idx[\"name\"] for idx in pc.list_indexes()]:\n",
        "    pc.delete_index(index_name)\n",
        "\n",
        "# Create Pinecone index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=dimension,\n",
        "    metric=\"euclidean\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "\n",
        "pinecone_index = pc.Index(index_name)\n",
        "\n",
        "# --- Load Data ---\n",
        "# Create folders & download a sample doc (kept same logic, fixed subfolder creation)\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "\n",
        "# --- Create Index ---\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Initialize OpenAI LLM\n",
        "llm = OpenAI(model=\"gpt-5.2-2025-12-11\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, llm=llm\n",
        ")\n",
        "\n",
        "# --- System Prompt (polite + answer-from-document constraint) ---\n",
        "SYSTEM_PROMPT = \"\"\"You are Aisha, a polite and professional Insurance assistant.\n",
        "Answer ONLY using the information found in the indexed insurance document(s).\n",
        "If the answer is not in the document(s), say: \"I couldnâ€™t find that in the document.\"\n",
        "Keep responses concise, helpful, and courteous.\n",
        "\"\"\"\n",
        "\n",
        "# --- Query Engine ---\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "def query_doc(user_question: str):\n",
        "    if not user_question or not user_question.strip():\n",
        "        return \"Please enter a question.\"\n",
        "    full_query = f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "User question:\n",
        "{user_question.strip()}\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = query_engine.query(full_query)\n",
        "        text = str(response).strip()\n",
        "        # Gentle post-processing to keep it brief/polite\n",
        "        return text if text else \"I couldnâ€™t find that in the document.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# --- Gradio UI (Professional look with logo, centered title) ---\n",
        "# Use the raw GitHub URL for proper image rendering.\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/itsmerajesh4990/AIpracticeandtraining/main/ENEC%20logo%20new.jpeg\"\n",
        "CUSTOM_CSS = \"\"\"\n",
        ".gradio-container { font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, 'Helvetica Neue', Arial; }\n",
        ".header-wrap {\n",
        "    display: grid;\n",
        "    grid-template-columns: 120px 1fr 120px;\n",
        "    align-items: center;\n",
        "    gap: 12px;\n",
        "    padding: 12px 0 8px;\n",
        "    border-bottom: 1px solid #eaeaea;\n",
        "}\n",
        ".header-logo { display:flex; align-items:center; justify-content:flex-start; }\n",
        ".header-logo img { height: 48px; object-fit: contain; }\n",
        ".header-title { text-align:center; }\n",
        ".header-title h1 {\n",
        "    margin: 0; font-weight: 700; font-size: 1.5rem; line-height: 1.2;\n",
        "}\n",
        ".header-spacer { height: 1px; }\n",
        ".section { padding-top: 8px; }\n",
        ".footer-note { text-align:center; font-size: 12px; color:#667085; padding: 8px 0 0; }\n",
        "label.svelte-1ipelgc, .label-wrap label { font-weight: 600; }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=CUSTOM_CSS, title=\"SOP Document QA (LlamaIndex + Pinecone)\") as demo:\n",
        "    # Header with logo (left) and centered title\n",
        "    with gr.Row(elem_classes=\"header-wrap\"):\n",
        "        with gr.Column(scale=0, elem_classes=\"header-logo\"):\n",
        "            gr.HTML(f'<img src=\"{LOGO_URL}\" alt=\"ENEC Logo\" />')\n",
        "        with gr.Column(scale=1, elem_classes=\"header-title\"):\n",
        "            gr.HTML(\"<h1>Insurance QA</h1>\")\n",
        "        with gr.Column(scale=0):\n",
        "            gr.HTML(\"\")  # right-side spacer\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"Ask questions based on the SOP Document \"\n",
        "        \"**Answers come only from the document**. If not found, Iâ€™ll say so.\"\n",
        "    )\n",
        "\n",
        "    with gr.Group(elem_classes=\"section\"):\n",
        "        inp = gr.Textbox(\n",
        "            label=\"Your question\",\n",
        "            placeholder=\"e.g., Ask SOP Question?\",\n",
        "            lines=2,\n",
        "        )\n",
        "        btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "        out = gr.Textbox(label=\"Answer\", lines=8)\n",
        "\n",
        "    btn.click(fn=query_doc, inputs=inp, outputs=out)\n",
        "    inp.submit(fn=query_doc, inputs=inp, outputs=out)\n",
        "\n",
        "    gr.Markdown('<div class=\"footer-note\">LlamaIndex + Pinecone â€¢ Demo</div>')\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "iZsatRldDujy"
      },
      "id": "iZsatRldDujy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replicate the Deplpyment hugging face\n",
        "https://huggingface.co/spaces/decodingdatascience/ddsinsurance1\n",
        "openai key\n"
      ],
      "metadata": {
        "id": "LFolx25Cj4sK"
      },
      "id": "LFolx25Cj4sK"
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "op8voXD_DENK",
        "collapsed": true
      },
      "id": "op8voXD_DENK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvcUQSngoCCw"
      },
      "id": "DvcUQSngoCCw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import gradio as gr\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "\n",
        "# OpenAI LLM\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# --- Logging ---\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "# --- Initialize Pinecone ---\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "index_name = \"quickstart\"\n",
        "dimension = 1536\n",
        "\n",
        "# Delete index if exists\n",
        "existing_indexes = [idx[\"name\"] for idx in pc.list_indexes()]\n",
        "if index_name in existing_indexes:\n",
        "    pc.delete_index(index_name)\n",
        "\n",
        "# Create Pinecone index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=dimension,\n",
        "    metric=\"euclidean\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "\n",
        "pinecone_index = pc.Index(index_name)\n",
        "\n",
        "# --- Load SOP Document ---\n",
        "# Place your SOP file inside ./data\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "\n",
        "# --- Create Index ---\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# --- System Prompt for SOP ---\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a technical assistant specializing in industrial equipment.\n",
        "Answer ONLY using the information found in the indexed SOP document:\n",
        "'SOP_ Troubleshooting Overheated Boiler Feed Pump (Pump P-201)'.\n",
        "\n",
        "If the answer is not in the document, say:\n",
        "\"I couldnâ€™t find that in the document.\"\n",
        "\n",
        "Keep responses concise, accurate, and professional.\n",
        "\"\"\"\n",
        "\n",
        "# --- Query Engine ---\n",
        "query_engine = index.as_query_engine(\n",
        "    system_prompt=SYSTEM_PROMPT\n",
        ")\n",
        "\n",
        "def query_doc(user_question: str):\n",
        "    if not user_question or not user_question.strip():\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    try:\n",
        "        response = query_engine.query(user_question.strip())\n",
        "        text = str(response).strip()\n",
        "\n",
        "        if not text:\n",
        "            return \"I couldnâ€™t find that in the document.\"\n",
        "\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# --- Gradio UI ---\n",
        "LOGO_URL = \"https://raw.githubusercontent.com/itsmerajesh4990/AIpracticeandtraining/main/ENEC%20logo%20new.jpeg\"\n",
        "\n",
        "CUSTOM_CSS = \"\"\"\n",
        ".gradio-container { font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, 'Helvetica Neue', Arial; }\n",
        ".header-wrap {\n",
        "    display: grid;\n",
        "    grid-template-columns: 120px 1fr 120px;\n",
        "    align-items: center;\n",
        "    gap: 12px;\n",
        "    padding: 12px 0 8px;\n",
        "    border-bottom: 1px solid #eaeaea;\n",
        "}\n",
        ".header-logo { display:flex; align-items:center; justify-content:flex-start; }\n",
        ".header-logo img { height: 48px; object-fit: contain; }\n",
        ".header-title { text-align:center; }\n",
        ".header-title h1 {\n",
        "    margin: 0; font-weight: 700; font-size: 1.5rem; line-height: 1.2;\n",
        "}\n",
        ".section { padding-top: 8px; }\n",
        ".footer-note { text-align:center; font-size: 12px; color:#667085; padding: 8px 0 0; }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=CUSTOM_CSS, title=\"SOP QA (LlamaIndex + Pinecone)\") as demo:\n",
        "    with gr.Row(elem_classes=\"header-wrap\"):\n",
        "        with gr.Column(scale=0, elem_classes=\"header-logo\"):\n",
        "            gr.HTML(f'<img src=\"{LOGO_URL}\" alt=\"ENEC Logo\" />')\n",
        "        with gr.Column(scale=1, elem_classes=\"header-title\"):\n",
        "            gr.HTML(\"<h1>SOP Troubleshooting QA</h1>\")\n",
        "        with gr.Column(scale=0):\n",
        "            gr.HTML(\"\")\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"Ask questions based on the SOP document for Pump Pâ€‘201. \"\n",
        "        \"**Answers come only from the document**.\"\n",
        "    )\n",
        "\n",
        "    with gr.Group(elem_classes=\"section\"):\n",
        "        inp = gr.Textbox(\n",
        "            label=\"Your question\",\n",
        "            placeholder=\"e.g., What is the first step when the pump overheats?\",\n",
        "            lines=2,\n",
        "        )\n",
        "        btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "        out = gr.Textbox(label=\"Answer\", lines=8)\n",
        "\n",
        "    btn.click(fn=query_doc, inputs=inp, outputs=out)\n",
        "    inp.submit(fn=query_doc, inputs=inp, outputs=out)\n",
        "\n",
        "    gr.Markdown('<div class=\"footer-note\">LlamaIndex + Pinecone â€¢ Demo</div>')\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "yL39sIgkk4M9"
      },
      "id": "yL39sIgkk4M9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}